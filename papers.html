<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Lei Wu</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="papers.html" class="current">Publications</a></div>
<div class="menu-item"><a href="./cv.pdf">CV</a></div>
</td>
<td id="layout-content">
<p>* indicates  equal contribution or alphabetical author order.</p>
<h1>Published</h1>
<ul>
<li><p><b>Towards a mathematical understanding of neural network-based machine learning: what we know and what we don't</b> <br />
Weinan E*, Chao Ma*, Stephan Wojtowytsch*, Lei Wu* <br />
CSIAM Trans. Appl. Math., 2020. [<a href="https://arxiv.org/abs/2009.10713">arXiv</a>] [<a href="https://doi.org/10.4208/csiam-am.SO-2020-0002">journal</a>]</p>
</li>
</ul>
<ul>
<li><p><b>Machine learning based non-Newtonian fluid model with molecular fidelity</b> <br />
Huan Lei, Lei Wu, Weinan E<br />
Physical Review E, 2020. [<a href="https://arxiv.org/abs/2003.03672">arXiv</a>] [<a href="https://doi.org/10.1103/PhysRevE.102.043309">journal</a>]</p>
</li>
</ul>
<ul>
<li><p><b>Machine learning from a continuous viewpoint, I</b><br />
Weinan E*, Chao Ma*, Lei Wu* <br />
Science China Mathematics, 2020. [<a href="https://arxiv.org/abs/1912.12777">arXiv</a>] [<a href="https://doi.org/10.1007/s11425-020-1773-8">journal</a>]</p>
</li>
</ul>
<ul>
<li><p><b>The slow deterioration of the generalization error of the random feature model</b> <br />
Chao Ma*, Lei Wu*, Weinan E<br />
Mathematical and Scientific Machine Learning (MSML), 2020. [<a href="https://arxiv.org/abs/2008.05621">arXiv</a>] [<a href="http://proceedings.mlr.press/v107/ma20a.html">proceedings</a>]</p>
</li>
</ul>
<ul>
<li><p><b>Barron spaces and flow-induced function spaces for neural network models</b> <br />
Weinan E*, Chao Ma*, Lei Wu* <br /> 
Constructive Approximation, 2020 (In press). [<a href="https://arxiv.org/abs/1906.08039">arXiv</a>]</p>
</li>
</ul>
<ul>
<li><p><b>A comparative analysis of the optimization and generalization property of two-layer neural network and random feature models under gradient descent dynamics</b><br />
Weinan E*, Chao Ma*, Lei Wu* <br />
Science China Mathematics, 2020. [<a href="https://arxiv.org/abs/1904.04326">arXiv</a>] [<a href="https://doi.org/10.1007/s11425-019-1628-5">journal</a>]</p>
</li>
</ul>
<ul>
<li><p><b>The generalization error of minimum-norm solutions for over-parameterized neural networks</b><br />
Weinan E*, Chao Ma*, Lei Wu* <br />
Journal of Pure and Applied Functional Analysis, 2020. [<a href="https://arxiv.org/abs/1912.06987">arXiv</a>] [<a href="http://www.yokohamapublishers.jp/online2/oppafa/vol5/p1445.html">journal</a>]</p>
</li>
</ul>
<ul>
<li><p><b>Global convergence of gradient descent for deep linear residual networks</b><br />
Lei Wu*, Qingcan Wang*, Chao Ma<br />
Neural Information Processing Systems (NeurIPS), 2019. [<a href="https://arxiv.org/abs/1911.00645">arXiv</a>] [<a href="https://papers.nips.cc/paper/2019/hash/14da15db887a4b50efe5c1bc66537089-Abstract.html">proceedings</a>]</p>
</li>
</ul>
<ul>
<li><p><b>A priori estimates of the population risk for two-layer neural networks</b><br />
Weinan E*, Chao Ma*, Lei Wu*<br /> 
Communications in Mathematical Sciences, 2019. [<a href="https://arxiv.org/abs/1810.06397">arXiv</a>] [<a href="https://dx.doi.org/10.4310/CMS.2019.v17.n5.a11">journal</a>]</p>
</li>
</ul>
<ul>
<li><p><b>The anisotropic noise in stochastic gradient descent: Its behavior of escaping from minima and regularization effects</b> <br />
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, Jinwen Ma<br />
International Conference on Machine Learning (ICML), 2019. [<a href="https://arxiv.org/abs/1803.00195">arXiv</a>] [<a href="http://proceedings.mlr.press/v97/zhu19e.html">proceedings</a>]</p>
</li>
</ul>
<ul>
<li><p><b>How SGD selects the global minima in over-parameterized learning: A stability perspective</b> <br />
Lei Wu*, Chao Ma, Weinan E<br /> 
Advances in Neural Information Processing Systems (NeurIPS), 2018. [<a href="http://papers.nips.cc/paper/8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning-a-dynamical-stability-perspective">proceedings</a>]</p>
</li>
</ul>
<ul>
<li><p><b>Understanding and enhancing the transferability of adversarial examples</b><br />
Lei Wu*, Zhanxing Zhu<br />
Asian Conference on Machine Learning (ACML), 2020. [<a href="https://arxiv.org/abs/1802.09707">arXiv</a>] [<a href="http://proceedings.mlr.press/v129/wu20a.html">proceedings</a>]</p>
</li>
</ul>
<ul>
<li><p><b>Irreversible samplers from jump and continuous Markov processes</b> <br />
Yi-An Ma, Emily B Fox, Tianqi Chen, Lei Wu<br />
Statistics and Computing, 2018. [<a href="https://arxiv.org/abs/1608.05973">arXiv</a>] [<a href="https://doi.org/10.1007/s11222-018-9802-x">journal</a>]</p>
</li>
</ul>
<ul>
<li><p><b>Towards understanding generalization of deep learning: perspective of loss landscapes</b> <br />
Lei Wu*, Zhanxing Zhu, Weinan E<br />
Workshop on Principled Approaches to Deep Learning, ICML2017. [<a href="https://arxiv.org/abs/1706.10239">arXiv</a>]</p>
</li>
</ul>
<ul>
<li><p><b>Smoothed dissipative particle dynamics model for mesoscopic multiphase flows in the presence of thermal fluctuations</b><br />
Huan Lei, Nathan A Baker, Lei Wu, Gregory K Schenter, Christopher J Mundy, Alexandre M Tartakovsky<br />
Physical Review E, 2016. [<a href="https://arxiv.org/pdf/1512.00138.pdf">arXiv</a>] [<a href="https://doi.org/10.1103/PhysRevE.94.023304">journal</a>]</p>
</li>
</ul>
<h1>Preprints</h1>
<ul>
<li><p><b>Complexity measures for neural networks with general activation functions using path-based norms</b> <br />
Zhong Li*, Chao Ma*, Lei Wu*<br />
arxiv preprint, 2020. [<a href="https://arxiv.org/abs/2009.06132">arXiv</a>]</p>
</li>
</ul>
<ul>
<li><p><b>The quenching-activation behavior of the gradient descent dynamics for two-layer neural network models</b> <br />
Chao Ma*, Lei Wu*, Weinan E<br />
arXiv Preprint, 2020. [<a href="https://arxiv.org/abs/2006.14450">arXiv</a>]</p>
</li>
</ul>
<ul>
<li><p><b>Analysis of the gradient descent algorithm for a deep neural network model with skip-connections</b> <br />
Weinan E*, Chao Ma*, Lei Wu* <br />
arXiv preprint, 2019 [<a href="https://arxiv.org/abs/1904.05263">arXiv</a>]</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2021-01-28 09:43:22 EST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
