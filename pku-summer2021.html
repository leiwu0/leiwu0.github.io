<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Lei Wu</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="papers.html">Publications</a></div>
<div class="menu-item"><a href="teaching.html" class="current">Teaching</a></div>
<div class="menu-item"><a href="./cv.pdf">CV</a></div>
</td>
<td id="layout-content">
<h3></h3>
<h1>Theoretical Deep Learning</h1>
<h2>Lecture notes</h2>
<ol>
<li><p><a href="lecture-note/lec-1.pdf">A brief introduction to supervised learning</a></p>
</li>
<li><p><a href="lecture-note/lec-2.pdf">Concentration inequalities</a></p>
<ol>
<li><p>Sub-Gaussian, Chernoff bound, Hoeffding's inequality, McDiarmid's inequalty</p>
</li></ol>
</li>
<li><p><a href="lecture-note/lec-3.pdf">Uniform bounds and empirical processes</a></p>
<ol>
<li><p>Rademacher complexity, Covering number, Dudley entropy integral</p>
</li></ol>
</li>
<li><p><a href="lecture-note/lec-4.pdf">Kernel methods, representer theorem and RKHSs</a></p>
</li>
<li><p><a href="lecture-note/lec-5.pdf">RKHS II</a></p>
</li>
<li><p><a href="lecture-note/lec-6.pdf">Two-layer neural networks and the Fourier analysis</a></p>
</li>
<li><p><a href="lecture-note/lec-7.pdf">The Barron space</a></p>
</li>
<li><p>Deep neural networks</p>
<ol>
<li><p><a href="lecture-note/lec-8.pdf">lecture note</a>,</p>
</li>
<li><p><a href="lecture-note/resnet-approx.pdf">Approximation theory of deep ResNets</a> </p>
</li>
<li><p><a href="lecture-note/slide-depth-separation.pdf">Depth separations</a></p>
</li></ol>
</li>
<li><p>Training neural networks: Convergence?</p>
<ol>
<li><p><a href="lecture-note/gd-brief-overview.pdf">A brief overview of GD convergence</a></p>
</li>
<li><p><a href="lecture-note/slide-9.pdf">Slide</a></p>
</li></ol>
</li>
<li><p>Training neural networks beyond the kernel regime</p>
<ol>
<li><p><a href="lecture-note/slide-10.pdf">Slide</a></p>
</li>
</ol>

</li>
</ol>
<h2>References</h2>
<ul>
<li><p>Peter Bartlett's course: <a href="https://www.stat.berkeley.edu/~bartlett/courses/2014spring-cs281bstat241b/">Statistical Learning Theory</a></p>
</li>
<li><p>Matus Telgarsky's notes: <a href="https://mjt.cs.illinois.edu/dlt/">Deep Learning Theory Lecture Notes</a></p>
</li>
<li><p>S. Shalev-Shwartz and S Ben-David, <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/copy.html">Understanding Machine Learning: From Theory to Algorithms</a>,  2014. </p>
</li>
<li><p>Roman Vershynin, <a href="https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html">High-dimensional probability: An introduction with applications in data science</a>, 2018.</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2021-08-05, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
