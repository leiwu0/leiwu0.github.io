<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Publications</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Lei Wu</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="publication.html" class="current">Publications</a></div>
<div class="menu-item"><a href="course.html">Courses</a></div>
<div class="menu-item"><a href="group.html">Group</a></div>
<div class="menu-item"><a href="https://pku-fomi.github.io">Seminar</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Publications</h1>
</div>
<p>* indicates  equal contribution;<br /> 
α-β indicates alphabetical author order.
</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2501.14475" target=&ldquo;blank&rdquo;>Point Cloud Neural Operator for Parametric PDEs on Complex and Variable Geometries</a><br />
Chenyu Zeng, Yanshu Zhang, Jiayi Zhou, Yuhan Wang, Zilin Wang, Yuhao Liu, <b>Lei Wu</b>, Daniel Zhengyu Huang<br />
<b>Comput. Methods Appl. Mech. Eng., 2025</b>
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2502.19002" target=&ldquo;blank&rdquo;>The Sharpness Disparity Principle in Transformers for Accelerating Language Model Pre-Training</a><br />
Jinbo Wang*, Mingze Wang*, Zhanpeng Zhou*, Junchi Yan, Weinan E, <b>Lei Wu</b><br /> 
<b>ICML 2025</b>.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2503.06001" target=&ldquo;blank&rdquo;>Analyzing the role of permutation invariance in linear mode connectivity</a> <br />
Keyao Zhan*, Puheng Li*, <b>Lei Wu</b><br /> 
<b>AISTATS 2025</b>.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.05642" target=&ldquo;blank&rdquo;>A duality framework for analyzing random feature  and two-layer neural networks</a><br />
(α-β) Hongrui Chen, Jihao Long, <b>Lei Wu</b><br />
<b>Annals of Statistics</b> (to appear), <b>2025</b>.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.11474" target=&ldquo;blank&rdquo;>How Transformers Get Rich: Approximation and Dynamics Analysis</a><br />
Mingze Wang, Ruoxi Yu, <b>Lei Wu</b><br />
arXiv preprint arXiv:2410.11474
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2405.20763" target=&ldquo;blank&rdquo;>Improving generalization and convergence by enhancing implicit regularization</a><br />
Mingze Wang, Jinbo Wang, Haotian He,  Zilin Wang, Guanhua Huang, Feiyu Xiong, Zhiyu Li, <b>Lei Wu</b><br />
<b>NeurIPS 2024</b>.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2404.06391" target=&ldquo;blank&rdquo;>Exploring neural network landscapes: star-shaped and geodesic connectivity</a><br />
Zhanran Lin*, Puheng Li*, <b>Lei Wu</b><br />
arxiv preprint, 2024.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2402.15718" target=&ldquo;blank&rdquo;>Optimal Rates and Saturation for Noiseless Kernel Ridge Regression</a><br />
(α-β) Jihao Long, Xiaojun Peng, <b>Lei Wu</b><br />
arxiv preprint, 2024.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2402.07193" target=&ldquo;blank&rdquo;>Parameter symmetry and noise equilibrium of stochastic gradient descent</a><br />
Liu Ziyin, Mingze Wang, Li HongChao, <b>Lei Wu</b><br />
<b>NeurIPS 2024</b>.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2407.12332" target=&ldquo;blank&rdquo;>Why do you grok? A theoretical analysis on grokking modular addition</a><br /> 
Mohamad Amin Mohamadi, Zhiyuan Li, <b>Lei Wu</b>, Danica J. Sutherland<br />
<b>ICML 2024</b>.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2311.15221" target=&ldquo;blank&rdquo;>The local landscape of phase retrieval under limited samples</a><br /> 
Kaizhao Liu*, Zihao Wang*, <b>Lei Wu</b><br /> 
<b>IEEE Trans. Inform. Theory</b>, 2024.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2311.14387" target=&ldquo;blank&rdquo;>Achieving margin maximization exponentially fast via progressive norm rescaling</a><br /> 
Mingze Wang, Zeping Min, <b>Lei Wu</b><br /> 
<b>ICML 2024</b>.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2310.00692" target=&ldquo;blank&rdquo;>The noise geometry of stochastic gradient descent: A quantitative and analytical characterization</a><br />
Mingze Wang, <b>Lei Wu</b><br /> 
arXiv preprint, 2023.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2306.02833" target=&ldquo;blank&rdquo;>The L^infty learnability of reproducing kernel Hilbert spaces</a><br />
(α-β) Hongrui Chen, Jihao Long, <b>Lei Wu</b><br />
arXiv preprint, 2023.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.19082" target=&ldquo;blank&rdquo;>Embedding inequalities for Barron-type spaces</a><br /> 
<b>Lei Wu</b><br />
<b>Journal of Machine Learning</b>, 2023.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.08404" target=&ldquo;blank&rdquo;>Theoretical analysis of inductive biases in deep convolutional networks</a><br />
Zihao Wang, <b>Lei Wu</b><br /> 
<b>NeurIPS 2023</b>.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.17490" target=&ldquo;blank&rdquo;>The implicit regularization of dynamical stability in stochastic gradient descent</a><br />
<b>Lei Wu</b>, Weijie J. Su<br />
<b>ICML 2023</b>.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2207.02628" target=&ldquo;blank&rdquo;>The alignment property of SGD noise and how it helps select flat minima: A stability analysis</a><br />
<b>Lei Wu</b>, Mingze Wang, Weijie J. Su<br />
<b>NeurIPS 2022</b>.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://doc.global-sci.org/uploads/Issue/JML/v1n3/13_247.pdf?code=YEsY82RTD%2BI4klud4NqN0w%3D%3D" target=&ldquo;blank&rdquo;>Beyond the quadratic approximation: the multiscale structure of neural network loss landscapes</a><br />
Chao Ma, Daniel Kunin, <b>Lei Wu</b>, Lexing Ying<br />
<b>Journal of Machine Learning</b>, 2022.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2108.04964" target=&ldquo;blank&rdquo;>A spectral-based analysis of the separation between two-layer neural networks and linear methods</a><br /> 
<b>Lei Wu</b>, Jihao Long <br /> 
<b>JMLR 2022</b>.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2202.08064" target=&ldquo;blank&rdquo;>Learning a single neuron for non-monotonic activation functions</a> <br /> 
<b>Lei Wu</b> <br /> 
<b>AISTATS 2022</b>.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://msml21.github.io/papers/id24.pdf" target=&ldquo;blank&rdquo;>A qualitative study of the dynamic behavior of adaptive gradient algorithms</a> <br />
Chao Ma*, <b>Lei Wu</b>*, Weinan E<br />
<b>Mathematical and Scientific Machine Learning (MSML), 2021</b>.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2009.10713" target=&ldquo;blank&rdquo;>Towards a mathematical understanding of neural network-based machine learning: what we know and what we don't</a> <br />
(α-β) Weinan E, Chao Ma, Stephan Wojtowytsch, <b>Lei Wu</b> <br />
<b>CSIAM Trans. Appl. Math.</b>, 2020.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2006.14450" target=&ldquo;blank&rdquo;>The quenching-activation behavior of the gradient descent dynamics for two-layer neural network models</a> <br />
Chao Ma*, <b>Lei Wu</b>*, Weinan E<br />
arXiv Preprint, 2020. 
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2003.03672" target=&ldquo;blank&rdquo;>Machine learning based non-Newtonian fluid model with molecular fidelity</a> <br />
Huan Lei, <b>Lei Wu</b>, Weinan E<br />
<b>Physical Review E</b>, 2020.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1912.12777" target=&ldquo;blank&rdquo;>Machine learning from a continuous viewpoint, I</a><br />
(α-β) Weinan E, Chao Ma, <b>Lei Wu</b> <br />
<b>Sci. China Math.</b>, 2020.
</p>
</li>
</ul>
<ul>
<li><p><a href="http://proceedings.mlr.press/v107/ma20a.html" target=&ldquo;blank&rdquo;>The slow deterioration of the generalization error of the random feature model</a> <br />
Chao Ma*, <b>Lei Wu</b>*, Weinan E<br />
Mathematical and Scientific Machine Learning (MSML), 2020.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1906.08039" target=&ldquo;blank&rdquo;>The Barron space and flow-induced function spaces for neural network models</a> <br />
(α-β) Weinan E, Chao Ma, <b>Lei Wu</b> <br /> 
<b>Constructive Approximation</b>, 2021.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2009.06132" target=&ldquo;blank&rdquo;>Complexity measures for neural networks with general activation functions using path-based norms</a> <br />
(α-β) Zhong Li, Chao Ma, <b>Lei Wu</b> <br />
arxiv preprint, 2020. 
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1904.04326" target=&ldquo;blank&rdquo;>A comparative analysis of the optimization and generalization property of two-layer neural network and random feature models under gradient descent dynamics</a><br />
(α-β) Weinan E, Chao Ma, <b>Lei Wu</b><br />
<b>Sci. China Math.</b>, 2020.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1904.05263" target=&ldquo;blank&rdquo;>Analysis of the gradient descent algorithm for a deep neural network model with skip-connections</a> <br />
(α-β) Weinan E, Chao Ma, <b>Lei Wu</b> <br />
arXiv preprint, 2019.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1912.06987" target=&ldquo;blank&rdquo;>The generalization error of minimum-norm solutions for over-parameterized neural networks</a><br />
(α-β) Weinan E, Chao Ma, <b>Lei Wu</b> <br />
Journal of Pure and Applied Functional Analysis, 2020.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1911.00645" target=&ldquo;blank&rdquo;>Global convergence of gradient descent for deep linear residual networks</a> <br />
<b>Lei Wu</b>*, Qingcan Wang*, Chao Ma<br />
<b>NeurIPS 2019</b>.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1810.06397" target=&ldquo;blank&rdquo;>A priori estimates of the population risk for two-layer neural networks</a> <br />
(α-β) Weinan E, Chao Ma, Lei Wu<br /> 
<b>Communications in Mathematical Sciences</b>, 2019.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1803.00195" target=&ldquo;blank&rdquo;>The anisotropic noise in stochastic gradient descent: Its behavior of escaping from minima and regularization effects</a> <br />
Zhanxing Zhu, Jingfeng Wu, Bing Yu, <b>Lei Wu</b>, Jinwen Ma<br />
<b>ICML 2019</b>.
</p>
</li>
</ul>
<ul>
<li><p><a href="http://papers.nips.cc/paper/8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning-a-dynamical-stability-perspective" target=&ldquo;blank&rdquo;>How SGD selects the global minima in over-parameterized learning: A dynamical stability perspective</a> <br />
<b>Lei Wu</b>, Chao Ma, Weinan E<br /> 
<b>NeurIPS 2018</b>.
</p>
</li>
</ul>
<ul>
<li><p><a href="http://proceedings.mlr.press/v129/wu20a.html" target=&ldquo;blank&rdquo;>Towards understanding and improving the transferability of adversarial examples in deep neural networks</a> <br />
<b>Lei Wu</b>, Zhanxing Zhu<br />
<b>ACML, 2020</b> [<a href="https://arxiv.org/abs/1802.09707" target=&ldquo;blank&rdquo;>arXiv version</a>].
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1608.05973" target=&ldquo;blank&rdquo;>Irreversible samplers from jump and continuous Markov processes</a> <br />
Yi-An Ma, Emily B Fox, Tianqi Chen, <b>Lei Wu</b><br />
<b>Statistics and Computing</b>, 2018.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1706.10239" target=&ldquo;blank&rdquo;>Towards understanding generalization of deep learning: perspective of loss landscapes</a> <br />
<b>Lei Wu</b>, Zhanxing Zhu, Weinan E<br />
<b>Workshop on Principled Approaches to Deep Learning</b>, ICML2017. 
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1512.00138.pdf" target=&ldquo;blank&rdquo;>Smoothed dissipative particle dynamics model for mesoscopic multiphase flows in the presence of thermal fluctuations</a> <br />
Huan Lei, Nathan A Baker, <b>Lei Wu</b>, Gregory K Schenter, Christopher J Mundy, Alexandre M Tartakovsky<br />
<b>Physical Review E</b>, 2016.
</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2025-05-27 15:09:38 CST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
