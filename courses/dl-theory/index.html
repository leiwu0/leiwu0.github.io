<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">DL-Theory</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
</td>
<td id="layout-content">
<h3></h3>
<h1>Topics in Deep Learning Theory</h1>
<h2>Course Description </h2>
<p>This course builds a rigorous theoretical foundation for modern machine learning, tracing ideas from classical linear models to todayâ€™s large-scale neural networks. Through four connected modules, you will master the core mathematical tools, landmark results, and open research questions that shape deep-learning theory.
</p>
<h2>Prerequisites</h2>
<ul>
<li><p>Completion of the course <a href="../mi2ml/index.html" target=&ldquo;blank&rdquo;><i>Mathematical Introduction to Machine Learning</i></a> or familiarity with machine learning and deep learning concepts. A formal background in machine learning theory is not required. 
</p>
</li>
</ul>
<ul>
<li><p>This course is mathematically rigorous and requires a solid background in linear algebra, mathematical analysis, and familiarity  with high-dimensional probability and functional analysis. Please ensure you have the necessary preparation before enrolling.
</p>
</li>
</ul>
<h2>Lecture Notes</h2>
<ul>
<li><p>Part I: Foundational Framework
</p>
<ul>
<li><p><a href="./lecture-00.pdf" target=&ldquo;blank&rdquo;>Lecture 0: Concentration Inequalities</a>
</p>
</li>
<li><p><a href="./lecture-01.pdf" target=&ldquo;blank&rdquo;>Lecture 1: Generalization via Uniform Concentration</a>
</p>
</li></ul>
</li>
<li><p>Part II: Classical  Theory
</p>
<ul>
<li><p><a href="./lecture-02.pdf" target=&ldquo;blank&rdquo;>Lecture 2: Linear Methods for Regression</a>
</p>
</li>
<li><p><a href="./lecture-03.pdf" target=&ldquo;blank&rdquo;>Lecture 3: Reproducing Kernel Hilbert Spaces</a>
</p>
</li>
<li><p><a href="./lecture-04.pdf" target=&ldquo;blank&rdquo;>Lecture 4: Theoretical Analysis of KRR</a>
</p>
</li>
<li><p><a href="./lecture-05.pdf" target=&ldquo;blank&rdquo;>Lecture 5: Learning in RKHS  with SGD</a>
</p>
</li>
<li><p><a href="./lecture-06.pdf" target=&ldquo;blank&rdquo;>Lecture 6: Random Feature Models</a>
</p>
</li></ul>
</li>
<li><p>Part III: Deep Learning Theory
</p>
<ul>
<li><p><a href="./lecture-07.pdf" target=&ldquo;blank&rdquo;>Lecture 7: Two-Layer Neural Networks</a>
</p>
</li>
<li><p><a href="./lecture-08.pdf" target=&ldquo;blank&rdquo;>Lecture 8: Optimization of Neural Networks</a>
</p>
</li>
<li><p><a href="./lecture-09.pdf" target=&ldquo;blank&rdquo;>Lecture 9: Deep Neural Networks</a>
</p>
</li>
<li><p><a href="./lecture-10.pdf" target=&ldquo;blank&rdquo;>Lecture 10: Implicit Bias/Regularization I</a>
</p>
</li>
<li><p><a href="./lecture-11.pdf" target=&ldquo;blank&rdquo;>Lecture 11: How GD/SGD Converges to Flat Minima</a>
</p>
</li>
<li><p><a href="./lecture-12.pdf" target=&ldquo;blank&rdquo;>Lecture 12: GD Converges to Max-Margin Solutions</a>
</p>
</li></ul>
</li>
<li><p>Part IV: Frontier Topics
</p>
<ul>
<li><p><a href="./lecture-13.pdf" target=&ldquo;blank&rdquo;>Lecture 13: Feature Learning</a>
</p>
</li>
<li><p><a href="./lecture-14.pdf" target=&ldquo;blank&rdquo;>Lecture 14: Neural Network Landscape</a>
</p>
</li>
<li><p><a href="./lecture-15.pdf" target=&ldquo;blank&rdquo;>Lecture 15: Emerging Phenomena in LLM</a>
</p>
</li>
</ul>

</li>
</ul>
<h2>Relevant Books and Courses</h2>
<ul>
<li><p>Introduction to ML from a theoretical perspective: 
</p>
<ul>
<li><p>Shai Shalev-Shwartz, Shai Ben-David, <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning" target=&ldquo;blank&rdquo;>Understanding Machine Learning: From Theory to Algorithms</a>.
</p>
</li>
<li><p>Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar, <a href="https://cs.nyu.edu/~mohri/mlbook/" target=&ldquo;blank&rdquo;>Foundations of Machine Learning</a>.	
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>More theoretical textbooks:
</p>
<ul>
<li><p>Francis Bach, <a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf" target=&ldquo;blank&rdquo;>Learning Theory from First Principles</a>.
</p>
</li>
<li><p>Roman Vershynin, <a href="https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html" target=&ldquo;blank&rdquo;>High-Dimensional Probability</a>	
</p>
</li>
<li><p>Ramon van Handel, <a href="https://web.math.princeton.edu/~rvan/APC550.pdf" target=&ldquo;blank&rdquo;>Probability in High Dimension</a>.
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Deep Learning Theory Courses:
</p>
<ul>
<li><p>Matus Telgarsky, <a href="https://mjt.cs.illinois.edu/dlt/two.pdf" target=&ldquo;blank&rdquo;>Deep Learning Theory</a>.
</p>
</li>
<li><p>Joan Bruna's course: <a href="https://cims.nyu.edu/~bruna/teaching/" target=&ldquo;blank&rdquo;>Mathematics of Deep Learning</a>
</p>
</li>
</ul>

</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2025-05-26, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
