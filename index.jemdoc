# jemdoc: menu{MENU}{index.html}
# jemdoc: addcss{jemdoc.css}, notime
= Lei Wu (吴磊)



~~~
{}{img_left}{leiwu.jpeg}{Lei Wu}{120 px}{160 px}
\n
Assistant Professor \n 
[https://www.math.pku.edu.cn/ School of Mathematical Sciences] \n 
[https://cmlr.pku.edu.cn// Center for Machine Learning Research] \n 
[https://english.pku.edu.cn/ Peking University]

Office: 静园6院  205\n
Email: leiwu (at) math (dot) pku (dot) edu (dot) cn\n

[https://scholar.google.com/citations?user=CMweeYcAAAAJ&hl=en Google Scholar] ~~~ [https://github.com/leiwu0 Github]~~~~~ [./cv.pdf CV]\n
~~~


== About Me
I am  currently an Assistant Professor in the [https://www.math.pku.edu.cn School of Mathematical Sciences] and  [https://cmlr.pku.edu.cn Center for Machine Learning Research] at [https://www.pku.edu.cn/ Peking University]. 

Previously, I was a postdoc in PACM  at  Princeton University and in the Wharton Statistics and Data Science Department at the University of Pennsylvania.  I completed my Ph.D. in computational mathematics at Peking University in 2018, advised by  Prof. Weinan E. I received my B.S. degree in  mathematics from  Nankai University in 2012. 


My research aims to understand the mechanisms behind the success of deep learning, with a particular focus on:

- The approximation and representation power of neural networks

- The dynamical behavior of popular optimization algorithms such as SGD and Adam

- Emergent phenomena in the training of large language models (LLMs)

== Recruiting
We are actively seeking self-motivated postdocs, PhD students and interns to join our team. If you are interested in collaborating with me, please send an email to me with your CV and transcript as well as a brief description of your research interests.


== Selected Works
- Understanding and improving LLM pre-training 
	-- Design better optimizer: [https://arxiv.org/abs/2405.20763 NeurIPS 2024], [https://arxiv.org/abs/2502.19002 ICML 2025]

- A stability theory of implicit regularization
	-- Stability ensures SGD only select flat minima: [http://papers.nips.cc/paper/8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning-a-dynamical-stability-perspective NeurIPS 2028], [https://arxiv.org/abs/2207.02628 NeurIPS 2022]
	-- Flat minima can provably generalize well: [https://arxiv.org/abs/2305.17490 ICML 2023]
	-- Discovery of the Edge of Stability (EoS) phenomenon: [http://papers.nips.cc/paper/8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning-a-dynamical-stability-perspective NeurIPS 2028]

- Approximation theory for machine learning 
	-- Kernel and random features: [https://arxiv.org/abs/2305.05642 AoS 2025], [https://arxiv.org/abs/2402.15718 arXiv 2024]
	-- Barron space theory for neural networks: [https://arxiv.org/abs/1810.06397 CMS 2019], [https://arxiv.org/abs/1906.08039 Contr. Appr. 2021],  [https://arxiv.org/abs/2108.04964 JMLR 2022], [https://arxiv.org/abs/2305.19082 JML 2023], [https://arxiv.org/abs/2305.05642 AoS 2025]
	-- Deep CNNs: [https://arxiv.org/abs/2305.08404 NeurIPS 2023]




