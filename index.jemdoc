# jemdoc: menu{MENU}{index.html}
# jemdoc: addcss{jemdoc.css}, notime
= Lei Wu (吴磊)



~~~
{}{img_left}{leiwu.jpeg}{Lei Wu}{120 px}{160 px}
\n
Assistant Professor \n 
[https://www.math.pku.edu.cn/ School of Mathematical Sciences] \n 
[https://cmlr.pku.edu.cn// Center for Machine Learning Research] \n 
[https://english.pku.edu.cn/ Peking University]

Office: 静园6院  205\n
Email: leiwu (at) math (dot) pku (dot) edu (dot) cn\n

[https://scholar.google.com/citations?user=CMweeYcAAAAJ&hl=en Google Scholar] ~~~ [https://github.com/leiwu0 Github]~~~~~ [./cv.pdf CV]\n
~~~


== About Me
I am  currently an Assistant Professor in the [https://www.math.pku.edu.cn School of Mathematical Sciences] and  [https://cmlr.pku.edu.cn Center for Machine Learning Research] at [https://www.pku.edu.cn/ Peking University]. 

Previously, I was a postdoc in PACM  at  Princeton University and in the Wharton Statistics and Data Science Department at the University of Pennsylvania.  I completed my Ph.D. in computational mathematics at Peking University in 2018, advised by  Prof. Weinan E. I received my B.S. degree in  mathematics from  Nankai University in 2012. 


My research aims to understand the mechanisms behind the success of deep learning, with a particular focus on:

- The approximation and representation power of neural networks

- The dynamical behavior of popular optimization algorithms such as SGD and Adam

- Emergent phenomena in the training of large language models (LLMs)

== Recruiting
We are actively seeking self-motivated postdocs, PhD students and interns to join our team. If you are interested in collaborating with me, please send an email to me with your CV and transcript as well as a brief description of your research interests.


== Selected Works
- *Understanding and improving LLM pre-training* 
	-- Optimizer design: 
		--- [https://arxiv.org/abs/2405.20763 AdmIRE] & [https://arxiv.org/abs/2502.19002 Blockwise LR]: Speed up convergence by amplifying the dynamics along flat directions.
		--- [https://arxiv.org/abs/2505.24275 GradPower]: Make gradients more informative for faster convergence, requiring only a single-line code change.

- *A stability theory of implicit regularization*
	-- SGD prefers flat minima via stability: Introduced the *stability-based view of flatness bias*: [http://papers.nips.cc/paper/8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning-a-dynamical-stability-perspective NeurIPS 2018]
	--  The anisotropic  SGD noise is crucial for sharpness control.: [https://arxiv.org/abs/2207.02628 NeurIPS 2022]
	-- Flat minima provably generalize well for two-layer ReLU and diagonal linear nets: [https://arxiv.org/abs/2305.17490 ICML 2023]
	-- Stability-inspired algorithms for seeking flatter minima: [https://arxiv.org/abs/2405.20763 NeurIPS 2024]
	--  Discovery of the *edge of stability (EoS)* phenomenon: [http://papers.nips.cc/paper/8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning-a-dynamical-stability-perspective NeurIPS 2018]  (Table 2)



- *Approximation theory for machine learning*
	-- Approximation is the dual of estimation, and vice versa: [https://arxiv.org/abs/2305.05642 AoS 2025]
	-- Kernel and random features: [https://arxiv.org/abs/2402.15718 arXiv 2024]
	-- *Barron space* theory for neural networks: [https://arxiv.org/abs/1810.06397 CMS 2019], [https://arxiv.org/abs/1906.08039 Contr. Appr. 2021]
		--- A spectral analysis: [https://arxiv.org/abs/2108.04964 JMLR 2022]
		--- Embedding theorems: [https://arxiv.org/abs/2305.19082 JML 2023]
	-- Deep CNNs: [https://arxiv.org/abs/2305.08404 NeurIPS 2023]




