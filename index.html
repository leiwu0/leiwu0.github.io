<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Lei Wu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="course.html">Courses</a></div>
<div class="menu-item"><a href="group.html">Group</a></div>
<div class="menu-item"><a href="https://pku-fomi.github.io/">Seminar</a></div>
</td>
<td id="layout-content">
<h1>Lei Wu (吴磊)</h1>
<table class="imgtable"><tr><td>
<img src="leiwu.jpeg" alt="Lei Wu" width="120 px" height="160 px" />&nbsp;</td>
<td align="left"><p><br />
Assistant Professor <br /> 
<a href="https://www.math.pku.edu.cn/" target=&ldquo;blank&rdquo;>School of Mathematical Sciences</a> <br /> 
<a href="https://cmlr.pku.edu.cn//" target=&ldquo;blank&rdquo;>Center for Machine Learning Research</a> <br /> 
<a href="https://english.pku.edu.cn/" target=&ldquo;blank&rdquo;>Peking University</a>
</p>
<p>Office: 静园6院  205<br />
Email: leiwu (at) math (dot) pku (dot) edu (dot) cn<br />
</p>
<p><a href="https://scholar.google.com/citations?user=CMweeYcAAAAJ&amp;hl=en" target=&ldquo;blank&rdquo;>Google Scholar</a> &nbsp;&nbsp;&nbsp; <a href="https://github.com/leiwu0" target=&ldquo;blank&rdquo;>Github</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="./cv.pdf" target=&ldquo;blank&rdquo;>CV</a><br />
</p>
</td></tr></table>
<h2>About Me</h2>
<p>I am currently an Assistant Professor in the <a href="https://www.math.pku.edu.cn" target=&ldquo;blank&rdquo;>School of Mathematical Sciences</a> and
<a href="https://cmlr.pku.edu.cn" target=&ldquo;blank&rdquo;>Center for Machine Learning Research</a> at <a href="https://www.pku.edu.cn/" target=&ldquo;blank&rdquo;>Peking University</a>.
</p>
<p>Previously, I was a postdoc in PACM at Princeton University and in the Wharton Statistics and Data Science Department at the University of Pennsylvania.
I completed my Ph.D. in computational mathematics at Peking University in 2018, advised by Prof. Weinan E.
I received my B.S. degree in mathematics from Nankai University in 2012.
</p>
<p>My research aims to understand the mechanisms behind the success of deep learning, with a particular focus on:
</p>
<ul>
<li><p>The approximation and representation power of neural networks
</p>
</li>
<li><p>The dynamical behavior of popular optimization algorithms such as SGD and Adam
</p>
</li>
<li><p>Emergent phenomena in the training of large language models (LLMs)
</p>
</li>
</ul>
<h2>Recruiting</h2>
<p>We are actively seeking self-motivated <span class="red">postdocs</span>, <b>PhD students</b>, and <b>undergraduate interns</b> to join our team.
If you are interested in collaborating with me, please email me your CV, transcript, and a brief description of your research interests.
</p>
<h2>Research Highlights</h2>
<p>(See also: <a href="publication.html" target=&ldquo;blank&rdquo;>Full publication list</a>)
</p>
<ul>
<li><p><b>Understanding scaling laws.</b>
</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2509.19189" target=&ldquo;blank&rdquo;>Functional scaling laws (FSL) for kernel regression and LLM pre-training (NeurIPS 2025, Spotlight)</a>
</p>
</li>
<li><p><a href="https://arxiv.org/abs/2602.14208" target=&ldquo;blank&rdquo;>Optimal batch-size scheduling under FSL (ICLR 2026)</a>
</p>
</li>
<li><p><a href="https://arxiv.org/abs/2602.06797" target=&ldquo;blank&rdquo;>Optimal learning-rate scheduling under FSL (arXiv 2026)</a>
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>LLM pre-training.</b>
</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2502.19002" target=&ldquo;blank&rdquo;>Accelerate training via the sharpness disparity in transformer</a> 
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Implicit regularization, dynamical stability, and flatness bias.</b><br />
</p>
<ul>
<li><p><a href="http://papers.nips.cc/paper/8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning-a-dynamical-stability-perspective" target=&ldquo;blank&rdquo;>A dynamical stability perspective of implicit regularization (NeurIPS 2018)</a> (Discover the <b>edge of stability (EoS)</b> phenomenon; see Table 2)
</p>
</li>
<li><p><a href="https://arxiv.org/abs/2207.02628" target=&ldquo;blank&rdquo;>Alignment property of SGD noise &amp; flat minima selection (NeurIPS 2022)</a>
</p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.17490" target=&ldquo;blank&rdquo;>Stable and flat minima provably generalize well (ICML 2023)</a>
</p>
</li>
<li><p><a href="https://arxiv.org/abs/2405.20763" target=&ldquo;blank&rdquo;>Enhancing implicit regularization for generalization &amp; convergence (NeurIPS 2024)</a>
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Approximation theory of  machine learning.</b><br />
</p>
<ul>
<li><p><a href="https://arxiv.org/abs/1810.06397" target=&ldquo;blank&rdquo;>A priori population-risk estimates for two-layer networks (CMS 2019)</a>
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1906.08039" target=&ldquo;blank&rdquo;>Barron space and flow-induced function spaces (Constructive Approximation 2021)</a>
</p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.08404" target=&ldquo;blank&rdquo;>Inductive biases in deep convolutional networks (NeurIPS 2023)</a>
</p>
</li>
<li><p><a href="https://arxiv.org/abs/2402.15718" target=&ldquo;blank&rdquo;>Sobolev-norm convergence of kernel ridge regression (arXiv 2024)</a>
</p>
</li>
<li><p><a href="https://arxiv.org/abs/2305.05642" target=&ldquo;blank&rdquo;>Duality framework for random features and two-layer networks (AoS 2025)</a>
</p>
</li>
</ul>

</li>
</ul>
<h2>Recent News</h2>
<ul>
<li><p>2026-02: <a href="https://arxiv.org/abs/2602.19691" target=&ldquo;blank&rdquo;>Constant-depth network with smooth activations</a> released on arXiv.
</p>
<ul>
<li><p>Establishes that smooth activations (e.g., GELU, SiLU) enable smoothness adaptivity in constant-depth neural networks, achieving optimal approximation and statistical rates.
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>2026-02: <a href="https://arxiv.org/abs/2602.06797" target=&ldquo;blank&rdquo;>Optimal learning-rate schedules under FSL</a> released on arXiv.
</p>
<ul>
<li><p>proves that the optimal learning-rate schedule depends explicitly on task difficulty; in the hard-task regime, the optimal schedule necessarily exhibits a <a href="https://arxiv.org/abs/2404.06395" target=&ldquo;blank&rdquo;>warmup–stable–decay (WSD)</a> structure.
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>2026-02: <a href="https://arxiv.org/abs/2602.14208" target=&ldquo;blank&rdquo;>Fast catch-up, late switching</a> accepted to <b>ICLR 2026</b>.
</p>
<ul>
<li><p>studies batch-size scheduling under FSL, revealing a fast catch-up effect, which holds across linear regression and LLM pre-training.
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>2025-09: <a href="https://arxiv.org/abs/2509.19189" target=&ldquo;blank&rdquo;>Functional Scaling Laws</a> accepted to <b>NeurIPS 2025 (Spotlight)</b>.
</p>
<ul>
<li><p>introduces a functional scaling laws (FSL) framework that—in contrast to classical scaling laws, which only describe final-step behavior—characterizes the entire loss trajectory, spanning from linear regression to LLM pre-training.</p>
</li>
</ul>

</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2026-03-01, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
